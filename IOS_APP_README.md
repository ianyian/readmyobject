# my1314 - iOS App

An educational iOS application that uses custom YOLOv8 machine learning model to demonstrate AI object detection and counting in real-time.

**Educational Purpose:** This app is designed for learning and exploring YOLOv8 COCO object detection technology.

## üéØ Features

### Core Functionality

- üì∏ **Camera Capture**: Take photos using iPhone back camera
- üñºÔ∏è **Photo Library**: Analyze existing photos from library
- ü§ñ **AI Detection**: Powered by custom YOLOv8 Core ML model
- üé® **Visual Results**: Bounding boxes with color-coded labels
- üìä **Detailed Counts**: Total chips and breakdown by color
- ‚ö° **Fast Processing**: Optimized for iOS Neural Engine

### Detected Chip Colors (Educational Model)

- ‚ö´ **Black** chips
- üü¢ **Green** chips
- üî¥ **Red** chips
- ‚ö™ **White-Blue** chips

## üì± Screenshots & UI

### Main Screen

- Welcome screen with app branding
- Two main action buttons:
  - "Take Photo" (blue) - Opens camera
  - "Choose from Library" (green) - Opens photo picker
- Color legend showing detectable chip types

### Detection Results Screen

- Original image with overlaid bounding boxes
- Each detection labeled with:
  - Chip color name
  - Confidence percentage
- Summary card showing:
  - Total chips detected
  - Breakdown by color with icons
  - Processing time

## üèóÔ∏è Architecture

### SwiftUI + MVVM Pattern

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          ContentView.swift          ‚îÇ  Main UI & Navigation
‚îÇ  (Coordinator for camera & results) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CameraView.swift      ‚îÇ  ‚îÇ ChipDetectionVM      ‚îÇ
‚îÇ PhotoPicker.swift     ‚îÇ  ‚îÇ (Vision + Core ML)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ best.mlpackage      ‚îÇ
                           ‚îÇ (YOLOv8 Core ML)    ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Components

#### **Models/** (`ChipDetection.swift`)

- `Detection` - Single chip detection with bounding box, color, confidence
- `ChipColor` - Enum for chip types with display properties
- `DetectionResult` - Complete detection session with statistics

#### **ViewModels/** (`ChipDetectionViewModel.swift`)

- Loads Core ML model
- Processes images using Vision framework
- Converts model output to Detection objects
- Handles errors and loading states

#### **Views/**

- `CameraView.swift` - UIKit camera wrapper for SwiftUI
- `DetectionResultView.swift` - Results display with bounding boxes
- `ContentView.swift` - Main app coordinator

## üõ†Ô∏è Technical Details

### Requirements

- **iOS:** 15.0+
- **Xcode:** 14.0+
- **Swift:** 5.0+
- **Frameworks:**
  - SwiftUI
  - Vision
  - Core ML
  - UIKit (Camera)

### Model Integration

- **Format:** Core ML (.mlpackage)
- **Input:** 640√ó640 RGB image
- **Output:** Bounding boxes with class labels
- **Inference:** Vision framework with VNCoreMLRequest
- **Hardware:** Neural Engine, GPU, or CPU

### Performance

- **Inference Time:** 0.1-0.5 seconds (device dependent)
- **Frame Rate:** Suitable for real-time video (20-60 FPS)
- **Model Size:** ~6-50 MB (depending on YOLOv8 variant)
- **Memory:** Minimal impact, runs on all modern iPhones

## üìÇ File Structure

```
readmyobject/
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îî‚îÄ‚îÄ ChipDetection.swift           # Data models
‚îú‚îÄ‚îÄ ViewModels/
‚îÇ   ‚îî‚îÄ‚îÄ ChipDetectionViewModel.swift  # Detection logic
‚îú‚îÄ‚îÄ Views/
‚îÇ   ‚îú‚îÄ‚îÄ CameraView.swift              # Camera capture
‚îÇ   ‚îî‚îÄ‚îÄ DetectionResultView.swift     # Results display
‚îú‚îÄ‚îÄ ContentView.swift                 # Main UI
‚îú‚îÄ‚îÄ readmyobjectApp.swift            # App entry point
‚îú‚îÄ‚îÄ Info.plist                        # Permissions
‚îî‚îÄ‚îÄ Assets.xcassets/                  # App assets
```

## üîí Privacy & Permissions

### Required Permissions

- **Camera**: Capture photos for chip detection
- **Photo Library**: Select existing photos to analyze

### Privacy Features

- ‚úÖ All processing done on-device
- ‚úÖ No data sent to servers
- ‚úÖ No image storage without user action
- ‚úÖ Clear permission descriptions

## üé® Design Highlights

### Color Scheme

- **Primary:** Blue (main actions)
- **Secondary:** Green (alternative actions)
- **Accent:** Color-coded by chip type
- **Background:** Light/adaptive

### Typography

- **Headers:** Bold system font
- **Body:** Regular system font
- **Labels:** Semibold for emphasis

### UI/UX

- Large, tappable buttons
- Clear visual hierarchy
- Loading indicators
- Error alerts
- Dismissible sheets

## üöÄ Setup & Installation

See [QUICK_START.md](QUICK_START.md) for 5-minute setup or [IOS_SETUP_GUIDE.md](IOS_SETUP_GUIDE.md) for detailed instructions.

**Quick Version:**

1. Convert model: `python convert_to_coreml.py`
2. Open Xcode: `open readmyobject.xcodeproj`
3. Add `best.mlpackage` to project
4. Add all Swift files to project
5. Build & Run

## üìä Model Details

### YOLOv8 Custom Educational Model

- **Purpose:** Educational demonstration of AI object detection
- **Trained on:** Custom chip dataset for learning purposes
- **Classes:** 4 (Black, Green, Red, White-Blue)
- **Architecture:** YOLOv8 (Ultralytics)
- **Accuracy:** ~90%+ mAP on validation set
- **Confidence Threshold:** 0.5 (configurable)
- **Technology:** YOLOv8 COCO-based detection

See [MODEL_INFO.md](MODEL_INFO.md) for complete model documentation.

## üß™ Testing

### Recommended Test Cases

1. **Lighting conditions:** Bright, indoor, low-light
2. **Camera angles:** Top-down, 45¬∞, side view
3. **Chip arrangements:** Scattered, stacked, mixed
4. **Distances:** Close-up, medium, far

### Test Data

Use validation images from `data/valid/images/`:

- Import to iOS photo library
- Test with "Choose from Library"
- Verify detection accuracy

## üêõ Known Limitations

- Overlapping objects may be counted as one
- Very low lighting reduces accuracy
- Extreme angles affect detection
- Minimum object size for detection
- Best results with top-down view

**Note:** This is an educational demonstration app for learning AI object detection concepts.

## üîÆ Future Enhancements

### Planned Features (Educational Improvements)

- [ ] Real-time video detection
- [ ] Adjustable confidence slider
- [ ] Save results to Photos
- [ ] Export as CSV/JSON
- [ ] Educational tutorials on AI detection
- [ ] Detection history
- [ ] More object types support
- [ ] Dark mode optimization
- [ ] iPad specific layout
- [ ] Share results feature

### Advanced Features

- [ ] AR overlay in camera view
- [ ] Stack counting (multiple layers)
- [ ] Currency conversion
- [ ] Multi-table support
- [ ] Batch processing
- [ ] Cloud sync (optional)

## üìù Code Examples

### Basic Usage

```swift
// Initialize view model
let viewModel = ChipDetectionViewModel()

// Detect chips in image
viewModel.detectChips(in: capturedImage)

// Access results
if let result = viewModel.detectionResult {
    print("Total chips: \(result.totalChips)")
    print("Red chips: \(result.chipCounts[.red] ?? 0)")
}
```

### Adjust Confidence

```swift
// In ChipDetectionViewModel
var confidenceThreshold: Float = 0.3  // More detections
var confidenceThreshold: Float = 0.7  // Higher accuracy
```

### Custom Processing

```swift
// Filter detections by color
let redChips = result.detections(for: .red)
let highConfidence = result.detections.filter { $0.confidence > 0.8 }
```

## ü§ù Contributing

Contributions welcome! Areas for improvement:

- UI/UX enhancements
- Performance optimizations
- Additional features
- Bug fixes
- Documentation

## üìÑ License

MIT License - See main [README.md](README.md)

## üôè Acknowledgments

- **YOLOv8:** Ultralytics
- **Core ML:** Apple
- **Vision Framework:** Apple
- **SwiftUI:** Apple

## üìû Support

- **Documentation:** [IOS_SETUP_GUIDE.md](IOS_SETUP_GUIDE.md)
- **Model Info:** [MODEL_INFO.md](MODEL_INFO.md)
- **Quick Start:** [QUICK_START.md](QUICK_START.md)
- **Issues:** GitHub Issues
- **Discussions:** GitHub Discussions

---

**Built with ‚ù§Ô∏è for AI learning and education**

_Version 1.0 - December 31, 2025_
